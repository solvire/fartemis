Fartemis Project Technical Checkpoint Template
Project Context
We're developing the LinkedIn integration for the Fartemis job hunting assistant. The main goal is to efficiently collect, store, and process job data from LinkedIn, with particular focus on finding early-stage companies hiring senior engineers. This integration is a critical component of our broader job search system.
Current Progress Summary

Feed System Architecture Overhaul

Simplified the FeedItem model to focus solely on raw data storage
Created a flexible feed source abstraction layer that can accommodate multiple job data sources
Implemented efficient database-centric storage instead of file-based approaches


LinkedIn Job Fetching Implementation

Created a robust command for fetching jobs directly from LinkedIn API
Developed a system to handle LinkedIn's unpredictable filtering behavior
Established a consistent data format with combined job summaries, details, and skills


Data Pipeline Development

Designed a processing flow from raw API data to structured job objects
Implemented a mapper system to transform LinkedIn data to our database models
Created mechanisms for identifying and avoiding duplicate job entries



Technical Implementation Details
FeedItem Model Structure
We significantly simplified the FeedItem model to focus on storing raw data before processing:
pythonCopyclass FeedItem(BaseIntModel):
    """Represents a raw job posting from a feed source before processing."""
    
    guid = models.CharField(max_length=255, help_text="Unique ID from source")
    source = models.ForeignKey(
        FeedSource, 
        related_name='items',
        on_delete=models.CASCADE
    )
    raw_data = models.JSONField(default=dict, blank=True, help_text="Original data from source")
    is_processed = models.BooleanField(default=False, help_text="Whether this item has been processed into a Job")
    job = models.ForeignKey(
        'jobboards.Job',
        on_delete=models.SET_NULL,
        related_name='feed_sources',
        null=True,
        blank=True
    )
    
    class Meta:
        app_label = 'jobboards'
        verbose_name = _('Feed Item')
        verbose_name_plural = _('Feed Items')
        unique_together = ('source', 'guid')  # Ensure no duplicates
        indexes = [
            models.Index(fields=['is_processed']),
        ]
    
    def __str__(self):
        title = self.raw_data.get('title', self.guid) if isinstance(self.raw_data, dict) else self.guid
        return f"{title} ({self.source.name})"
LinkedIn Job Fetch Command
We created a Django management command to fetch jobs from LinkedIn:
pythonCopyfrom django.core.management.base import BaseCommand
from django.conf import settings
from django.utils import timezone
import logging
from datetime import datetime
from linkedin_api import Linkedin
from fartemis.jobboards.models import FeedSource, FeedItem, JobSource

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    help = 'Fetch jobs from LinkedIn and store them directly as feed items'

    def add_arguments(self, parser):
        parser.add_argument('--keywords', type=str, default='Python Engineer', 
                            help='Search keywords')
        parser.add_argument('--geo-id', type=str,
                            help='LinkedIn GeoID (obtained from linkedin_geoid_finder command)')
        parser.add_argument('--location', type=str,
                            help='Location for job')
        parser.add_argument('--limit', type=int, default=25, 
                            help='Number of results to fetch')
        parser.add_argument('--levels', type=str, default='senior,manager,director', 
                            help='Experience levels (comma separated)')
        parser.add_argument('--verbose', action='store_true', 
                            help='Display full job descriptions')

    def handle(self, *args, **options):
        # Extract options
        keywords = options['keywords']
        geo_id = options['geo_id']
        location = options['location']
        limit = options['limit']
        levels = options['levels'].split(',') if options['levels'] else []
        verbose = options['verbose']

        # Get linkedin feed source
        feed_source = self._get_linkedin_feed_source()
        if not feed_source:
            self.stderr.write(self.style.ERROR('LinkedIn feed source not found. Check that a FeedSource with name "linkedin" exists.'))
            return

        # Initialize the LinkedIn client and fetch jobs
        api = self._initialize_linkedin_api()
        if not api:
            return
            
        # Search for jobs
        search_params = self._prepare_search_params(keywords, location, geo_id, levels, limit)
        jobs = self._fetch_jobs(api, search_params)
        
        # Process results
        self._process_job_results(api, jobs, feed_source, keywords, location, geo_id, levels)
Feed Source Structure
We use a single feed source for LinkedIn jobs:
pythonCopyclass FeedSource(BaseIntModel):
    """Feed source configuration."""
    
    name = models.CharField(max_length=100, unique=True)
    url = models.URLField(max_length=500)
    source_type = models.CharField(
        max_length=50,
        choices=[
            ('rss', 'RSS/Atom Feed'),
            ('hackernews', 'Hacker News Who Is Hiring'),
            ('reddit', 'Reddit'),
            ('custom', 'Custom Source'),
        ],
        default='rss'
    )
    is_active = models.BooleanField(default=True)
    last_fetched = models.DateTimeField(null=True, blank=True)
    fetch_interval_minutes = models.IntegerField(default=60)  # How often to fetch
    config = models.JSONField(default=dict, blank=True)  # Additional config params
LinkedInJobMapper Implementation
We've created a mapper that transforms raw LinkedIn data to our Job model:
pythonCopyclass LinkedInJobMapper(BaseJobMapper):
    """
    Mapper for LinkedIn job data.
    """
    
    def map_job(self, job_summary: Dict[str, Any], job_details: Optional[Dict[str, Any]] = None, user=None) -> Optional[Job]:
        # Extract job ID
        job_id = job_summary.get('jobId', job_summary.get('entityUrn', '').split(':')[-1])
        
        # Check for existing job
        existing_job = self.find_existing_job(JobSource.LINKEDIN, job_id, user)
        if existing_job:
            return existing_job
        
        # Extract data from job_details
        title = job_details.get('title', job_summary.get('title', 'Untitled Job'))
        
        # Get company details
        company_name = "Unknown Company"
        if job_details and 'companyDetails' in job_details:
            company_name = job_details['companyDetails'].get('name', 'Unknown Company')
        
        # Get location and remote status
        location = job_details.get('formattedLocation', '')
        remote = job_details.get('workRemoteAllowed', False)
        
        # Get description
        description = ""
        description_html = ""
        if job_details and 'description' in job_details:
            description = job_details['description'].get('text', '')
            description_html = self.extract_html_content(description)
        
        # Get posting date
        posted_date = None
        if job_details and 'listedAt' in job_details:
            posted_date = datetime.fromtimestamp(job_details['listedAt'] / 1000.0)
        
        # Get apply URL
        apply_url = job_details.get('applyUrl', f'https://www.linkedin.com/jobs/view/{job_id}/')
        
        # Extract skills, employment type, keywords
        skills = self.extract_skills(job_summary, job_details)
        employment_type = self.extract_employment_type(description)
        keywords = self.extract_keywords(title, description)
        
        # Create job record
        job = Job.objects.create(
            title=title,
            description=description,
            description_html=description_html,
            url=apply_url,
            company_name=company_name,
            location=location,
            remote=remote,
            source=JobSource.LINKEDIN,
            source_id=job_id,
            posted_date=posted_date,
            employment_type=employment_type,
            required_skills=skills,
            keywords=keywords,
            user=user
        )
        
        return job
Data Structures and API Responses
LinkedIn API Structure
LinkedIn's API returns job data in two different components:

Job Summary (from search results):
pythonCopy{
    'trackingUrn': 'urn:li:jobPosting:4182694556',
    'repostedJob': False,
    'title': 'Software Engineer III, AI/ML, YouTube',
    '$recipeTypes': ['com.linkedin.deco.recipe.anonymous.Anon1578943416'],
    'posterId': '29326500',
    '$type': 'com.linkedin.voyager.dash.jobs.JobPosting',
    'contentSource': 'JOBS_PREMIUM_OFFLINE',
    'entityUrn': 'urn:li:fsd_jobPosting:4182694556'
}

Job Details (from individual job pages):
pythonCopy{
    'dashEntityUrn': 'urn:li:fsd_jobPosting:4182694556',
    'companyDetails': {
        'name': 'Google',
        # Additional company details...
    },
    'jobState': 'LISTED',
    'description': {
        'text': 'Job description text...',
        # Additional description metadata...
    },
    'title': 'Software Engineer III, AI/ML, YouTube',
    'workRemoteAllowed': False,
    'formattedLocation': 'Mountain View, CA',
    'listedAt': 1741986856000,
    # Additional fields...
}


Combined FeedItem Raw Data Structure
We store the combined data in the FeedItem.raw_data field:
pythonCopy{
    'job_summary': {
        # Original job summary data from search results
    },
    'job_details': {
        # Original job details data from job page
    },
    'skills_data': {
        # Original skills data from LinkedIn API
    },
    'extracted_skills': [
        # List of extracted skill names
        'Python', 'Machine Learning', 'TensorFlow'
    ],
    'query_metadata': {
        'keywords': 'Python Engineer',
        'location': 'San Francisco',
        'geo_id': '12345',
        'timestamp': '20250320_123045',
        'levels': ['senior', 'manager'],
        'search_id': '20250320_123045'
    }
}
Integration Points
LinkedIn API to FeedItem
We fetch and store job data directly from LinkedIn to our database:
pythonCopy# Process a single job from LinkedIn
def _process_job(self, api, job, feed_source, search_timestamp, query_metadata):
    try:
        # Extract job ID
        job_id = job.get('jobId', job.get('entityUrn', '').split(':')[-1])
        
        if not job_id:
            self.stderr.write(f'Could not extract job ID from job')
            return None
        
        # Format the GUID to include the source
        guid = f"linkedin_{job_id}"
        
        # Check for existing item
        existing_item = FeedItem.objects.filter(
            source=feed_source,
            guid=guid
        ).first()
        
        if existing_item:
            self.stdout.write(f'Job {job_id} already exists in database, skipping...')
            return None
        
        # Get detailed job information
        self.stdout.write(f'Fetching details for job (ID: {job_id})...')
        details = api.get_job(job_id)
        
        # Try to get skills
        skills_data = None
        skills = []
        try:
            skills_data = api.get_job_skills(job_id)
            if skills_data and 'skillMatchStatuses' in skills_data:
                for skill_data in skills_data.get('skillMatchStatuses', []):
                    skill_name = skill_data.get('skill', {}).get('name', '')
                    if skill_name:
                        skills.append(skill_name)
        except Exception as e:
            self.stderr.write(f'Error fetching skills for job {job_id}: {str(e)}')
        
        # Create combined data object
        combined_data = {
            'job_summary': job,
            'job_details': details,
            'skills_data': skills_data,
            'extracted_skills': skills,
            'query_metadata': query_metadata
        }
        
        # Create FeedItem
        feed_item = FeedItem.objects.create(
            guid=guid,
            source=feed_source,
            raw_data=combined_data,
            is_processed=False
        )
        
        return feed_item
        
    except Exception as e:
        self.stderr.write(f'Error processing job: {str(e)}')
        return None
FeedItem to Job Transformation
The mapper transforms FeedItem data to Job objects:
pythonCopydef process_feed_items():
    """Process unprocessed feed items into Job objects."""
    
    unprocessed_items = FeedItem.objects.filter(is_processed=False)
    
    for item in unprocessed_items:
        try:
            # Determine the appropriate mapper
            if item.source.name == 'linkedin':
                mapper = LinkedInJobMapper()
            else:
                # Use a default mapper or skip
                continue
                
            # Extract data components from raw_data
            raw_data = item.raw_data
            job_summary = raw_data.get('job_summary', {})
            job_details = raw_data.get('job_details', {})
            
            # Map to Job object
            job = mapper.map_job(job_summary, job_details)
            
            if job:
                # Link the job back to the feed item
                item.job = job
                item.is_processed = True
                item.save()
        except Exception as e:
            logger.error(f"Error processing feed item {item.id}: {str(e)}")
Key Decisions and Rationale

Decision: Use a simplified FeedItem model focused only on raw data storage

Context: Initially, we had a more complex FeedItem model with redundant fields
Options Considered:

Option 1: Keep detailed fields in both FeedItem and Job models - Redundant, harder to maintain
Option 2: Use FeedItem only for temporary storage before creating Job objects - Simple but limited
Option 3: Use FeedItem with minimal fields focused on storing raw data - Efficient, flexible


Chosen Approach: Option 3 - Simplified FeedItem model
Rationale: This approach keeps the raw data intact for future reference while avoiding redundancy. It also makes the feed system more adaptable to different data sources.


Decision: Use "linkedin_<jobid>" as the guid format

Context: We needed a way to uniquely identify jobs across multiple sources
Options Considered:

Option 1: Use raw LinkedIn job IDs - Simple but could conflict with other sources
Option 2: Use prefixed IDs - More explicit source identification


Chosen Approach: Option 2 - Source-prefixed IDs
Rationale: This approach provides clarity about the source of each job and prevents potential ID collisions when we add more sources.


Decision: Store data directly in the database rather than in files

Context: Initially considered storing LinkedIn response data in timestamped files
Options Considered:

Option 1: File-based storage with timestamps - Traditional ETL approach
Option 2: Database-centric storage in JSON fields - Integrated with application


Chosen Approach: Option 2 - Database-centric storage
Rationale: Eliminates file I/O overhead, makes data immediately available to the application, and simplifies deployment by removing file system dependencies.



Challenges and Solutions

Challenge: LinkedIn API's unreliable filtering

Problem: Despite providing parameters for location, experience level, etc., LinkedIn's API often returns results that don't match these criteria
Solution: Implemented a "firehose" approach - collect broadly and filter locally
Code Example:
pythonCopy# Instead of relying on LinkedIn's filters, we fetch more data and filter ourselves
def _process_job_results(self, api, jobs, feed_source, keywords, location, geo_id, levels):
    search_timestamp = timezone.now().strftime('%Y%m%d_%H%M%S')
    jobs_found = len(jobs) if jobs else 0
    jobs_processed = 0
    
    # First, collect all the data
    for job in jobs:
        # Store in FeedItem
        # ...
        
    # Later, in the mapper, we apply our own filtering
    def filter_job_by_criteria(self, job_details, criteria):
        """Apply our own filtering logic to job details."""
        # Location filtering
        if criteria.get('location') and job_details.get('formattedLocation'):
            if not self._location_matches(job_details['formattedLocation'], criteria['location']):
                return False
                
        # Level filtering
        # ... additional filtering logic
        
        return True



Challenge: Minimizing database operations while avoiding duplicates

Problem: Checking for duplicates on every job would lead to many database queries
Solution: Used the unique_together constraint and efficient duplicate detection
Code Example:
pythonCopy# In the model
class Meta:
    unique_together = ('source', 'guid')  # Database-level constraint
    
# In the command
def _process_job(self, api, job, feed_source, search_timestamp, query_metadata):
    # Extract job ID and create guid
    job_id = job.get('jobId', job.get('entityUrn', '').split(':')[-1])
    guid = f"linkedin_{job_id}"
    
    # Efficient duplicate check
    existing_item = FeedItem.objects.filter(
        source=feed_source,
        guid=guid
    ).exists()  # Just check existence, don't fetch the object
    
    if existing_item:
        return None




Next Steps

Implement Job Processing Command

Create a management command to process FeedItems into Jobs
Develop a robust error handling and retry mechanism
Implement logging and monitoring for job processing


Enhance Company Profile Matching

Develop algorithms to match jobs to existing company profiles
Implement creation of new company profiles for unknown companies
Build company information enrichment from other sources


Create Advanced Filtering Pipeline

Implement NLP-based extraction of job attributes
Develop scoring system for job quality and relevance
Create classifiers for identifying early-stage companies


Add Additional Job Sources

Implement Hacker News "Who is Hiring" feed client
Add support for remote job board feeds
Build Reddit job board integration



References and Resources

Internal References:

FeedSource model: fartemis.jobboards.models.FeedSource
FeedItem model: fartemis.jobboards.models.FeedItem
Job model: fartemis.jobboards.models.Job
LinkedIn fetch command: fartemis.jobboards.management.commands.fetch_linkedin_jobs


External References:

LinkedIn API SDK: https://linkedin-api.readthedocs.io
GeoID API documentation: https://www.linkedin.com/jobs-guest/api/typeaheadHits



This checkpoint provides a comprehensive overview of our LinkedIn integration progress, with detailed code examples and architectural decisions. It should serve as a solid foundation for continuing the development in the next session.