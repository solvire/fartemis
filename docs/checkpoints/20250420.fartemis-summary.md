Fartemis Project Technical Checkpoint
April 20, 2025
Project Context
The LinkedIn profile discovery component is a critical part of the Fartemis job search assistant. Initially designed to simply find LinkedIn profiles, it evolved into a sophisticated system that handles complex scenarios like name changes, company transitions, and profile variations. The system needs to maintain high accuracy while respecting privacy concerns and API limitations.
Current Progress Summary
Search Integration

Successfully implemented multi-engine search using DuckDuckGo and Tavily

DuckDuckGo provides broad coverage without API key requirements
Tavily adds AI-powered relevance scoring and context understanding
Combined approach provides redundancy and improved accuracy


Created robust HTML parsing for DuckDuckGo results

Handles multiple result formats
Manages pagination automatically
Extracts structured data from semi-structured HTML


Integrated Tavily's AI-powered search capabilities

Leverages entity recognition
Provides relevance scoring
Handles context extraction


Overcame initial challenges with direct LinkedIn API access

Abandoned direct API approach due to restrictions
Developed alternative strategies using public data
Implemented rate limiting and request management



Profile Matching System
Scoring Engine
Developed sophisticated multi-factor scoring system:
pythondef _calculate_profile_match_score(self, url, context, first_name, last_name, company):
    """
    Calculate match score based on multiple factors
    
    Scoring Factors:
    - Name match in URL: 0-10 points
    - Name match in context: 0-6 points
    - Company match: 0-5 points
    - Special handle patterns: 0-10 points
    - Title relevance: 0-5 points
    - Content freshness: 0-3 points
    """
    score = 0
    
    # Name matching in URL (weighted heavily)
    name_url_score = self._calculate_name_match_in_url(url, first_name, last_name)
    score += name_url_score * 10
    
    # Context matching
    context_lower = context.lower()
    first_lower = first_name.lower()
    last_lower = last_name.lower()
    
    # Direct name matches in context
    if first_lower in context_lower:
        score += 3
    if last_lower in context_lower:
        score += 3
    
    # Company association (crucial for verification)
    if company:
        company_lower = company.lower()
        if company_lower in context_lower:
            score += 5
            # Extra points for company in professional context
            if any(term in context_lower for term in ['works at', 'employed by', 'joined']):
                score += 2
    
    # Special handle patterns
    handle = self._extract_handle_from_url(url)
    if handle:
        handle_lower = handle.lower()
        # Check various handle patterns
        patterns = [
            f"iam{first_lower}{last_lower}",
            f"{first_lower}{last_lower}",
            f"{first_lower}.{last_lower}",
            f"{first_lower}-{last_lower}"
        ]
        if any(pattern in handle_lower for pattern in patterns):
            score += 10
    
    # Title relevance
    title = self._extract_title_from_context(context)
    if title:
        title_lower = title.lower()
        if any(term in title_lower for term in ['manager', 'director', 'lead', 'head']):
            score += 5
    
    # Content freshness
    content_date = self._extract_date_from_context(context)
    if content_date:
        days_old = (datetime.now() - content_date).days
        if days_old < 30:
            score += 3
        elif days_old < 90:
            score += 2
        elif days_old < 180:
            score += 1
    
    return score
Name Change Detection System
Advanced name change detection with confidence scoring:
pythondef _analyze_name_changes(self, profile_data: Dict) -> Dict:
    """
    Analyze potential name changes with confidence scoring
    
    Returns:
        Dict containing:
        - original_name: The name we started with
        - current_name: The name found in profile
        - confidence: Score from 0-1
        - evidence: List of supporting evidence
    """
    result = {
        "original_name": f"{self.first_name} {self.last_name}",
        "current_name": None,
        "confidence": 0.0,
        "evidence": []
    }
    
    # Extract current name from profile
    current_name = profile_data.get("name")
    if not current_name:
        return result
    
    result["current_name"] = current_name
    
    # Calculate base confidence
    confidence = 0.0
    evidence = []
    
    # Check first name match
    if self.first_name.lower() in current_name.lower():
        confidence += 0.4
        evidence.append("First name match")
    
    # Check for company continuity
    if self._verify_company_timeline(profile_data):
        confidence += 0.3
        evidence.append("Continuous company history")
    
    # Check for professional connections
    shared_connections = self._analyze_professional_network(profile_data)
    if shared_connections > 0:
        confidence += min(0.2, shared_connections * 0.02)
        evidence.append(f"Found {shared_connections} shared professional connections")
    
    # Look for maiden name indicators
    if self._find_name_change_indicators(profile_data):
        confidence += 0.1
        evidence.append("Found explicit name change indicators")
    
    result["confidence"] = min(1.0, confidence)
    result["evidence"] = evidence
    
    return result
Data Management and Storage
Enhanced User Model
pythonclass User(AbstractUser):
    """
    Extended user model with comprehensive profile tracking
    """
    id = UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    first_name = CharField(max_length=145, null=True, blank=True)
    last_name = CharField(max_length=145, null=True, blank=True)
    middle_name = CharField(max_length=145, null=True, blank=True)
    email = EmailField(unique=True)
    linkedin_handle = CharField(max_length=255, null=True, blank=True)
    
    # Name history tracking
    alternate_names = JSONField(
        null=True, 
        blank=True,
        help_text="Historical names with timestamps and sources"
    )
    
    # Profile metadata
    profile_last_verified = DateTimeField(null=True, blank=True)
    profile_confidence_score = DecimalField(
        max_digits=3,
        decimal_places=2,
        default=0.0
    )
    
    # Search optimization
    search_keywords = ArrayField(
        CharField(max_length=100),
        null=True,
        blank=True
    )
    
    class Meta:
        indexes = [
            models.Index(fields=['linkedin_handle']),
            models.Index(fields=['email']),
            models.Index(fields=['last_name', 'first_name'])
        ]
User Contact Method Model
pythonclass UserContactMethod(BaseIntModel):
    """
    Stores multiple contact methods per user with verification
    """
    user = models.ForeignKey(
        settings.AUTH_USER_MODEL,
        on_delete=models.CASCADE,
        related_name='contact_methods'
    )
    method_type = models.ForeignKey(
        ContactMethodType,
        on_delete=models.PROTECT,
        related_name='user_methods'
    )
    value = models.CharField(max_length=255)
    is_primary = models.BooleanField(default=False)
    is_verified = models.BooleanField(default=False)
    verified_date = models.DateTimeField(null=True, blank=True)
    
    class Meta:
        unique_together = ('user', 'method_type', 'value')
        
    def save(self, *args, **kwargs):
        # Ensure only one primary contact per type
        if self.is_primary:
            UserContactMethod.objects.filter(
                user=self.user,
                method_type__category=self.method_type.category,
                is_primary=True
            ).update(is_primary=False)
        super().save(*args, **kwargs)
Integration Points
Employee Research Controller Integration
The EmployeeResearchController serves as the main orchestrator:
pythonclass EmployeeResearchController:
    """
    Orchestrates employee research and profile discovery
    """
    def __init__(self, company_profile: CompanyProfile, job_id: int = None,
                 search_llm_provider: str = LLMProvider.ANTHROPIC, 
                 extract_llm_provider: str = LLMProvider.MISTRAL):
        self.company = company_profile
        self.job = None if not job_id else Job.objects.get(id=job_id)
        
        # Initialize different LLMs for different tasks
        self.search_llm_client = LLMClientFactory.create(
            provider=search_llm_provider,
            default_params={
                "temperature": 0.2,
                "max_tokens": 2000
            }
        )
        
        self.extract_llm_client = LLMClientFactory.create(
            provider=extract_llm_provider,
            default_params={
                "temperature": 0.1,
                "max_tokens": 1000
            }
        )
        
        # Initialize profile finder
        self.linkedin_finder = LinkedInProfileFinder(verbose=False)
        
    def find_company_employees(self) -> List[User]:
        """Main method to find employees at the company"""
        found_employees = []
        
        # Research phases
        leadership = self._find_company_leadership()
        found_employees.extend(leadership)
        
        hiring_team = self._find_hiring_team()
        found_employees.extend(hiring_team)
        
        if self.job:
            job_specific = self._find_employees_for_job()
            found_employees.extend(job_specific)
        
        # Create or update user records
        return self._create_or_update_users(found_employees)


Key Decisions and Rationale
Decision: Multi-Engine Search Architecture
Context: Need reliable way to find LinkedIn profiles without direct API access while maintaining high accuracy and avoiding rate limiting.
Options Considered:

Direct LinkedIn API

python# Original attempted approach
class LinkedInAPIClient:
    def search_people(self, first_name, last_name, company):
        return linkedin.make_request(
            "GET",
            f"/v2/people-search?keywords={first_name}+{last_name}+{company}"
        )
Pros:

Direct access to profile data
Official API support
Cons:
Strict rate limiting
Limited access to data
High cost
Complex approval process


Web Scraping Approach

pythonclass LinkedInScraper:
    def scrape_profile(self, profile_url):
        response = requests.get(profile_url, headers=self.headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        return self._extract_profile_data(soup)
Pros:

Access to full profile data
No API costs
Cons:
High maintenance
Frequent breakage
Legal concerns
IP blocking


Multi-Engine Search (Chosen Approach)

pythonclass LinkedInProfileFinder:
    def __init__(self):
        self.search_engines = {
            'duckduckgo': DuckDuckGoSearch(),
            'tavily': TavilySearch(
                api_key=settings.TAVILY_API_KEY,
                max_results=10
            )
        }
    
    def find_profile(self, first_name, last_name, company=None):
        results = []
        for engine in self.search_engines.values():
            engine_results = engine.search(
                first_name=first_name,
                last_name=last_name,
                company=company
            )
            results.extend(engine_results)
        
        return self._process_results(results)
Pros:

More reliable through redundancy
Uses public data
Lower maintenance
Better scalability
Cons:
Requires result verification
More complex processing
Potential false positives

Decision: Profile Verification System
Context: Need to verify profile matches with high confidence while handling edge cases.
Options Considered:

Simple Name Matching

pythondef verify_profile(self, profile, first_name, last_name):
    return (
        first_name.lower() in profile.name.lower() and 
        last_name.lower() in profile.name.lower()
    )

Complex Scoring System (Chosen Approach)

pythonclass ProfileVerifier:
    def __init__(self):
        self.weights = {
            'name_match': 0.4,
            'company_match': 0.3,
            'title_match': 0.2,
            'connection_match': 0.1
        }
        
    def verify_profile(self, profile_data, search_criteria):
        score = 0
        evidence = []
        
        # Name matching with variations
        name_score = self._calculate_name_match(
            profile_data.get('name'),
            search_criteria.get('first_name'),
            search_criteria.get('last_name')
        )
        score += name_score * self.weights['name_match']
        
        # Company verification
        if self._verify_company(
            profile_data.get('company'),
            search_criteria.get('company')
        ):
            score += self.weights['company_match']
            evidence.append('Company match verified')
        
        # Title relevance
        title_score = self._calculate_title_relevance(
            profile_data.get('title'),
            search_criteria.get('role_type')
        )
        score += title_score * self.weights['title_match']
        
        # Network analysis
        connection_score = self._analyze_connections(
            profile_data.get('connections'),
            search_criteria.get('expected_network')
        )
        score += connection_score * self.weights['connection_match']
        
        return {
            'score': score,
            'evidence': evidence,
            'confidence': self._calculate_confidence(score)
        }
    
    def _calculate_name_match(self, profile_name, first_name, last_name):
        # Implementation of sophisticated name matching
        # including handling of:
        # - Maiden names
        # - Hyphenated names
        # - Cultural variations
        pass

AI-Based Matching

pythonclass AIProfileMatcher:
    def __init__(self):
        self.model = load_matching_model()
    
    def match_profile(self, profile_data, search_criteria):
        features = self._extract_features(profile_data, search_criteria)
        return self.model.predict_proba(features)[0]
Decision: Data Storage Architecture
Context: Need to store profile information and handle updates efficiently.
Chosen Approach: Hybrid storage with separate models for different aspects:
pythonclass UserProfile(models.Model):
    """Core user profile information"""
    user = models.OneToOneField(User, on_delete=models.CASCADE)
    linkedin_handle = models.CharField(max_length=255, unique=True)
    current_title = models.CharField(max_length=255)
    current_company = models.ForeignKey('Company', on_delete=models.SET_NULL)
    
    class Meta:
        indexes = [
            models.Index(fields=['linkedin_handle']),
            models.Index(fields=['current_company', 'current_title'])
        ]

class ProfileHistory(models.Model):
    """Track profile changes over time"""
    profile = models.ForeignKey(UserProfile, on_delete=models.CASCADE)
    field_name = models.CharField(max_length=100)
    old_value = models.TextField()
    new_value = models.TextField()
    change_date = models.DateTimeField(auto_now_add=True)
    
    class Meta:
        indexes = [
            models.Index(fields=['profile', 'change_date'])
        ]

class ProfileVerification(models.Model):
    """Track verification attempts and results"""
    profile = models.ForeignKey(UserProfile, on_delete=models.CASCADE)
    verification_date = models.DateTimeField(auto_now_add=True)
    verification_method = models.CharField(max_length=100)
    confidence_score = models.DecimalField(max_digits=3, decimal_places=2)
    evidence = models.JSONField()
    
    class Meta:
        indexes = [
            models.Index(fields=['profile', 'verification_date'])
        ]
Challenges and Solutions
Challenge: Rate Limiting and API Quotas
Problem: Different search engines have different rate limits and quotas.
Solution: Implemented adaptive rate limiting with fallback:
pythonclass AdaptiveRateLimiter:
    def __init__(self):
        self.limits = {
            'tavily': RateLimit(requests=100, period=60),  # 100 per minute
            'duckduckgo': RateLimit(requests=50, period=60)  # 50 per minute
        }
        self.usage = defaultdict(list)
    
    def can_make_request(self, engine):
        now = time.time()
        self._cleanup_old_requests(engine, now)
        
        limit = self.limits[engine]
        recent_requests = len(self.usage[engine])
        
        return recent_requests < limit.requests
    
    def record_request(self, engine):
        self.usage[engine].append(time.time())
    
    def _cleanup_old_requests(self, engine, now):
        limit = self.limits[engine]
        cutoff = now - limit.period
        
        self.usage[engine] = [
            t for t in self.usage[engine] 
            if t > cutoff
        ]

class SearchOrchestrator:
    def __init__(self):
        self.rate_limiter = AdaptiveRateLimiter()
        self.engines = ['tavily', 'duckduckgo']
    
    def search(self, query):
        results = []
        for engine in self.engines:
            if self.rate_limiter.can_make_request(engine):
                try:
                    engine_results = self._search_with_engine(engine, query)
                    results.extend(engine_results)
                    self.rate_limiter.record_request(engine)
                except RateLimitExceeded:
                    continue
        return results
Challenge: Name Variations and Changes
Problem: People's names change and have different representations.
Solution: Implemented a name variation generator and matcher:
pythonclass NameVariationGenerator:
    def __init__(self):
        self.common_prefixes = {'van', 'de', 'la', 'von', 'san'}
        self.common_suffixes = {'jr', 'sr', 'ii', 'iii', 'iv'}
    
    def generate_variations(self, first_name, last_name):
        variations = set()
        
        # Basic variations
        variations.add(f"{first_name} {last_name}")
        variations.add(f"{first_name[0]} {last_name}")
        variations.add(f"{first_name}.{last_name}")
        
        # Handle compound names
        if '-' in last_name:
            parts = last_name.split('-')
            variations.add(f"{first_name} {parts[0]}")
            variations.add(f"{first_name} {parts[1]}")
        
        # Handle prefixes
        words = last_name.lower().split()
        if len(words) > 1 and words[0] in self.common_prefixes:
            variations.add(f"{first_name} {' '.join(words[1:])}")
        
        return variations

class NameMatcher:
    def __init__(self):
        self.variation_generator = NameVariationGenerator()
    
    def match_names(self, name1, name2):
        """
        Calculate similarity between two names
        Returns score between 0 and 1
        """
        # Generate variations for both names
        variations1 = self.variation_generator.generate_variations(
            *self._split_name(name1)
        )
        variations2 = self.variation_generator.generate_variations(
            *self._split_name(name2)
        )
        
        # Calculate best match score
        best_score = max(
            self._calculate_similarity(v1, v2)
            for v1 in variations1
            for v2 in variations2
        )
        
        return best_score
    
    def _calculate_similarity(self, name1, name2):
        # Use Levenshtein distance for fuzzy matching
        distance = Levenshtein.distance(name1.lower(), name2.lower())
        max_length = max(len(name1), len(name2))
        return 1 - (distance / max_length)
Next Steps
1. Enhanced Name Matching
Technical Approach:
pythonclass EnhancedNameMatcher:
    def __init__(self):
        self.name_parser = NameParser()
        self.phonetic_encoder = PhoneticEncoder()
        
    def match_names(self, name1, name2):
        # Parse names into components
        parsed1 = self.name_parser.parse(name1)
        parsed2 = self.name_parser.parse(name2)
        
        # Generate phonetic encodings
        phonetic1 = self.phonetic_encoder.encode(name1)
        phonetic2 = self.phonetic_encoder.encode(name2)
        
        # Calculate various similarity metrics
        return {
            'exact_match': self._exact_match(parsed1, parsed2),
            'phonetic_match': self._phonetic_match(phonetic1, phonetic2),
            'component_match': self._component_match(parsed1, parsed2)
        }
2. Improved Company Context
Implementation Plan:
pythonclass CompanyContextAnalyzer:
    def __init__(self):
        self.industry_classifier = IndustryClassifier()
        self.org_structure_analyzer = OrgStructureAnalyzer()
        
    def analyze_context(self, company_name, title, description):
        # Analyze company industry
        industry = self.industry_classifier.classify(company_name)
        
        # Analyze organizational structure
        org_context = self.org_structure_analyzer.analyze(title)
        
        # Generate role-specific patterns
        return self._generate_patterns(industry, org_context)
3. Profile Verification Enhancement
Planned Implementation:
pythonclass EnhancedProfileVerifier:
    def __init__(self):
        self.email_verifier = EmailVerifier()
        self.connection_analyzer = ConnectionAnalyzer()
        self.activity_analyzer = ActivityAnalyzer()
        
    async def verify_profile(self, profile_data):
        # Run verifications in parallel
        results = await asyncio.gather(
            self.email_verifier.verify(profile_data.email),
            self.connection_analyzer.analyze(profile_data.connections),
            self.activity_analyzer.analyze(profile_data.activities)
        )
        
        return self._combine_results(results)
References and Resources
Internal References
Models:

users/models.py: User, UserProfile, ProfileHistory
companies/models.py: Company, CompanyRole
utils/linkedin_finder.py: LinkedInProfileFinder

Configuration:
pythonLINKEDIN_FINDER_CONFIG = {
    'search_engines': ['tavily', 'duckduckgo'],
    'rate_limits': {
        'tavily': {'requests': 100, 'period': 60},
        'duckduckgo': {'requests': 50, 'period': 60}
    },
    'confidence_thresholds': {
        'high': 0.8,
        'medium': 0.6,
        'low': 0.4
    }
}
API Endpoints:

POST /api/v1/profile/search/
GET /api/v1/profile/{id}/verify/
PUT /api/v1/profile/{id}/update/

External References
Libraries:

beautifulsoup4==4.9.3
python-Levenshtein==0.12.2
aiohttp==3.8.1
python-nameparser==1.0.6

API Documentation:

Tavily API: https://docs.tavily.com/
DuckDuckGo HTML Parameters: https://duckduckgo.com/params
LinkedIn Rate Limits: https://docs.microsoft.com/linkedin/shared/rate-limits

The implementation continues to evolve as we gather more real-world usage data and encounter new edge cases. Each component is designed to be modular and extensible, allowing us to enhance capabilities without major refactoring.