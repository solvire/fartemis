# Fartemis Project Technical Checkpoint
## Project Context

We're developing a robust job hunting and company research assistant focused on integrating job data from various sources, with particular emphasis on building intelligent tools to analyze companies, find their careers pages, and extract valuable information about potential employers. This enhances our ability to provide high-quality job and company recommendations to users seeking opportunities, especially for professionals who need deeper insights into company information.

## Current Progress Summary

### LinkedIn Job Integration System
- Implemented a streamlined FeedItem model for efficient raw data storage
- Created specialized commands for fetching LinkedIn job listings
- Developed a GeoID lookup system for enhanced geographical targeting
- Built mapping systems for jobs and companies to handle complex data transformations

### Company Research System
- Created a CompanyResearchController using LangChain and Claude AI integration
- Implemented intelligent career page discovery algorithms with pattern matching
- Developed structured information extraction for company profiles
- Added robust error handling for API interactions and data processing
- Built specialized URL handling with domain extraction and comparison logic

### Database and Model Structure
- Expanded the CompanyProfile model with comprehensive fields for company data
- Created CompanyResearchReferences model to track source information
- Added CompanyResearchLog for historical research activities
- Implemented relationship tracking between companies, jobs, and research data

## Technical Implementation Details

### CompanyResearchController Structure

```python
class CompanyResearchController:
    """Controller for company research operations"""
    
    def __init__(self):
        """Initialize the controller with necessary API clients"""
        # Make sure environment variables are set
        assert settings.TAVILY_API_KEY, "Tavily API key not found"
        assert settings.ANTHROPIC_API_KEY, "Anthropic API key not found"
        assert settings.LANGCHAIN_API_KEY, "LangChain API key not found"
        assert settings.LANGCHAIN_PROJECT, "LangChain project name not found"
        
        # Initialize LangSmith client
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_API_KEY"] = settings.LANGCHAIN_API_KEY
        os.environ["LANGCHAIN_PROJECT"] = settings.LANGCHAIN_PROJECT
        
        self.langsmith_client = Client()
        
        # Initialize LLM
        self.llm = init_chat_model("claude-3-5-haiku-20240307", model_provider="anthropic")
        
        # Initialize Tavily search tool
        self.tavily_search_tool = TavilySearch(
            max_results=10,
            topic="general",
            search_depth="advanced",
        )
        
        # Create React agent
        self.agent = create_react_agent(self.llm, [self.tavily_search_tool])
    
    def research_company(self, company_id: int) -> CompanyProfile:
        """Research a company using the React agent with Tavily search"""
        # Implementation...
```

### Career Page Detection Implementation

```python
import re

def _find_careers_page(self, company: CompanyProfile, references: List[Dict[str, str]]) -> str:
    """
    Find the careers page URL for a company from references or by searching
    """
    with langsmith.trace(
        name="Find Careers Page",
        run_type="chain"
    ) as run:
        # First, check if we already have it
        if company.careers_page_url:
            return company.careers_page_url
        
        # Try simple pattern matching if we have a website
        if company.website:
            # Normalize website URL
            base_url = company.website.rstrip('/')
            if not base_url.startswith(('http://', 'https://')):
                base_url = 'https://' + base_url
                
            # Try common patterns in order of likelihood
            patterns = [
                f"{base_url}/careers/jobs",
                f"{base_url}/careers",
                f"{base_url}/jobs",
                f"{base_url}/site/en-US/careers/jobs",
                f"{base_url}/site/en-US/careers",
                f"{base_url}/en-US/careers",
                f"{base_url}/en/careers"
            ]
            
            # Also try subdomain patterns
            domain_parts = urlparse(base_url).netloc.split('.')
            if len(domain_parts) >= 2:
                base_domain = '.'.join(domain_parts[-2:])
                subdomain_patterns = [
                    f"https://careers.{base_domain}",
                    f"https://jobs.{base_domain}"
                ]
                patterns.extend(subdomain_patterns)
            
            # Try each pattern
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            for url in patterns:
                try:
                    response = session.head(url, timeout=5)
                    if response.status_code < 400:  # Any success or redirect
                        run.add_metadata({
                            "source": "pattern_matching",
                            "url": url
                        })
                        return url
                except Exception:
                    continue
        
        # Check references
        for ref in references:
            url = ref.get("url", "").lower()
            title = ref.get("title", "").lower()
            
            careers_keywords = ["careers", "jobs", "work with us", "join us", "career", "job openings"]
            
            # Check if URL or title contains careers keywords
            is_careers_page = any(keyword in url for keyword in careers_keywords) or \
                              any(keyword in title for keyword in careers_keywords)
            
            # Check if it's on the company's official domain
            is_official_domain = False
            if company.website:
                company_domain = urlparse(company.website).netloc.lower()
                if company_domain.startswith('www.'):
                    company_domain = company_domain[4:]
                
                url_domain = urlparse(url).netloc.lower()
                if url_domain.startswith('www.'):
                    url_domain = url_domain[4:]
                
                # Check if domains match or if url_domain ends with company_domain
                is_official_domain = (url_domain == company_domain or 
                                     url_domain.endswith('.' + company_domain))
            
            # Check if it's NOT a specific job listing
            # Job listings often have numerical IDs at the end of the URL
            is_specific_job = bool(re.search(r'/jobs/\d+', url)) or bool(re.search(r'/careers/\d+', url))
            
            # Return if it meets our criteria
            if is_careers_page and (is_official_domain or company.name.lower() in url.lower()) and not is_specific_job:
                run.add_metadata({
                    "source": "references",
                    "url": url,
                    "is_official": is_official_domain
                })
                return url
        
        # Fall back to search and additional methods...
```

### CompanyProfile Model Structure

```python
class CompanyProfile(BaseIntModel):
    """
    Represents a company that could be a potential employer or target for applications.
    Stores core information about the company separate from any specific job listings.
    """
    name = models.CharField(max_length=255, verbose_name="Company Name")
    website = models.URLField(blank=True, null=True)
    careers_page_url = models.URLField(blank=True, null=True)
    description = models.TextField(blank=True, null=True)
    open_jobs_count = models.PositiveIntegerField(default=0)
    linkedin_id = models.CharField(max_length=255, blank=True)
    
    # Company size and details
    founded_year = models.PositiveIntegerField(blank=True, null=True)
    employee_count_min = models.PositiveIntegerField(blank=True, null=True) 
    employee_count_max = models.PositiveIntegerField(blank=True, null=True)
    
    # Location information
    headquarters_city = models.CharField(max_length=100, blank=True, null=True)
    headquarters_state = models.CharField(max_length=100, blank=True, null=True)
    headquarters_country = models.CharField(max_length=100, blank=True, null=True)
    
    # Business classification
    is_public = models.BooleanField(default=False)
    stock_symbol = models.CharField(max_length=10, blank=True, null=True)
    
    # AI-generated analysis and notes
    ai_analysis = models.TextField(blank=True, null=True)
    notes = models.TextField(blank=True, null=True)
    
    funding_status = models.CharField(max_length=100, choices=constants.FundingStatus.CHOICES, blank=True)
    funding_rounds = models.JSONField(default=list, blank=True)
    latest_funding_amount = models.DecimalField(max_digits=14, decimal_places=2, null=True, blank=True)
    latest_funding_date = models.DateField(null=True, blank=True)
    total_funding = models.DecimalField(max_digits=14, decimal_places=2, null=True, blank=True)
    
    # Sentiment data
    glassdoor_rating = models.DecimalField(max_digits=3, decimal_places=1, null=True, blank=True)
    indeed_rating = models.DecimalField(max_digits=3, decimal_places=1, null=True, blank=True)
    employee_sentiment_score = models.DecimalField(max_digits=3, decimal_places=1, null=True, blank=True)
    
    # Technical reputation
    github_repos = models.IntegerField(null=True, blank=True)
    github_stars = models.IntegerField(null=True, blank=True)
    open_source_contribution_score = models.DecimalField(max_digits=3, decimal_places=1, null=True, blank=True)
    
    # Continuous monitoring fields
    last_sentiment_update = models.DateTimeField(null=True, blank=True)
    last_funding_update = models.DateTimeField(null=True, blank=True)
    
    # Media and content
    recent_news = models.JSONField(default=list, blank=True)
    social_media_handles = models.JSONField(default=dict, blank=True)
    sentiment_data_points = models.JSONField(default=dict, blank=True)
```

### Research Models Structure

```python
class CompanyResearchReferences(BaseIntModel):
    """
    Links to external sources for company research
    """
    company = models.ForeignKey(
        CompanyProfile,
        on_delete=models.CASCADE,
        related_name='research_references'
    )
    title = models.CharField(max_length=255)
    url = models.URLField()
    content = models.TextField(blank=True, null=True)
    sentiment = models.CharField(max_length=100, choices=constants.CompanyReviewSentiment.CHOICES, blank=True)
    
    class Meta:
        verbose_name = "Company Research Reference"
        verbose_name_plural = "Company Research References"
    
    def __str__(self):
        return self.title + " - " + self.sentiment
    
class CompanyResearchLog(BaseIntModel):
    """
    Log of research activities and results of AI content generation for a company
    """
    company = models.ForeignKey(
        CompanyProfile,
        on_delete=models.CASCADE,
        related_name='research_logs'
    )

    content = models.TextField(blank=True, null=True)
```

## Data Structures and API Responses

### Tavily Search Results Structure

```python
{
    'results': [
        {
            'title': 'Company Name - Career Opportunities',
            'url': 'https://company.com/careers',
            'content': 'Text content from the page describing careers...',
            'score': 0.95
        },
        {
            'title': 'Company Name Jobs | LinkedIn',
            'url': 'https://linkedin.com/company/company-name/jobs',
            'content': 'Join our team of professionals...',
            'score': 0.85
        }
    ]
}
```

### LangChain React Agent Response Structure

```python
{
    'messages': [
        {
            'content': 'Comprehensive analysis of Company XYZ...'
        }
    ],
    'intermediate_steps': [
        [
            ActionNode(
                tool='tavily_search_tool',
                tool_input={'query': 'Company XYZ careers information'}
            ),
            [
                {'title': 'Search Result 1', 'url': 'http://example.com/1'},
                {'title': 'Search Result 2', 'url': 'http://example.com/2'}
            ]
        ]
    ]
}
```

## Integration Points

### Company Research to Database Integration

```python
def research_company(self, company_id: int) -> CompanyProfile:
    """
    Research a company using the React agent with Tavily search
    """
    # Get company profile
    company = CompanyProfile.objects.get(id=company_id)
    
    # Create LangSmith run
    with langsmith.trace(
        project_name=settings.LANGCHAIN_PROJECT,
        name=f"Company Research: {company.name}",
        tags=["company_research", f"company_{company.id}"]
    ) as run:
        # Execute research
        research_result, references = self._execute_research(query)
        
        # Store AI analysis
        company.ai_analysis = research_result
        company.last_sentiment_update = timezone.now()
        company.save()
        
        # Extract and save references
        self._save_references(company, references)
        
        # Log research activity
        self._log_research(company, research_result)
        
        # Extract structured data and update profile
        self._update_profile_from_analysis(company, research_result, references)
    
    return company
```

### LangChain Workflow Integration

```python
def _execute_research(self, query: str) -> tuple:
    """
    Execute research using React agent and extract references
    """
    with langsmith.trace(
        name="React Agent Execution",
        run_type="chain"
    ) as run:
        # Execute the agent
        agent_execution = self.agent.invoke({"messages": query})
        
        # Extract the final answer
        final_result = agent_execution['messages'][-1].content
        
        # Extract references from intermediate steps
        references = []
        steps = agent_execution.get('intermediate_steps', [])
        for step in steps:
            # Process steps to extract references...
        
        return final_result, references
```

## Key Decisions and Rationale

### Decision: Use of LangChain React Agent over custom implementation

**Context**: We needed a system to automatically research companies and extract structured information.

**Options Considered**:
- **Option 1**: Build a custom orchestration system - More control but slower development
- **Option 2**: Use LangChain's prebuilt React agent - Faster implementation but less control
- **Option 3**: Use direct API calls to Claude with no orchestration - Simplest but limited capabilities

**Chosen Approach**: Option 2 - Use LangChain's React agent

**Rationale**: The React agent provides a powerful "reasoning" approach where the AI can plan its research, execute searches, analyze results, and extract insights in a coordinated way. This saves significant development time while maintaining good quality and flexibility.

### Decision: Separate career page identification from full company research

**Context**: We needed to find company career pages reliably but in a lightweight way.

**Options Considered**:
- **Option 1**: Always run full company research - Comprehensive but heavy and expensive
- **Option 2**: Create a separate focused command for careers page discovery - Lighter but requires additional code
- **Option 3**: Use a simple pattern matching approach only - Fast but not very accurate

**Chosen Approach**: Option 2 - Create a separate lightweight command

**Rationale**: Career page discovery doesn't always require full company research. A separate command allows us to efficiently update just this information, saving API costs and processing time while maintaining accuracy.

## Challenges and Solutions

### Challenge: Identifying the correct careers page vs specific job listings

**Problem**: Our initial implementation was finding specific job listing URLs (e.g., `/careers/jobs/12345`) rather than the main careers page.

**Solution**: Implemented regex pattern matching to detect and filter out specific job listing URLs, and prioritize main careers pages.

**Code Example**:
```python
# Check if it's NOT a specific job listing
# Job listings often have numerical IDs at the end of the URL
is_specific_job = bool(re.search(r'/jobs/\d+', url)) or bool(re.search(r'/careers/\d+', url))

# Only return if it's not a specific job listing
if is_careers_page and is_official_domain and not is_specific_job:
    return url
```

### Challenge: Unreliable API responses from Tavily search

**Problem**: Tavily search sometimes returns results in different formats, causing our code to break when trying to access certain attributes.

**Solution**: Implemented robust normalization and validation of search results before processing them.

**Code Example**:
```python
def _normalize_search_results(self, search_results):
    """Helper to normalize different search result formats"""
    results_to_process = []
    
    # Check if search_results is a dictionary (single result or metadata container)
    if isinstance(search_results, dict):
        # If it has a 'results' key, use that
        if 'results' in search_results:
            results_to_process = search_results.get('results', [])
        # Otherwise, treat the dictionary itself as a single result
        else:
            results_to_process = [search_results]
    # Check if search_results is a list (multiple results)
    elif isinstance(search_results, list):
        results_to_process = search_results
    else:
        # If it's neither dict nor list, log the unexpected type
        logger.warning(f"Unexpected search_results type: {type(search_results)}")
    
    return results_to_process
```

## Next Steps

### Enhance Career Page Analysis

- Implement job listing count extraction from careers pages
- Add job keyword filtering to identify relevant positions (e.g., Python)
- Build URL pattern detection to scrape job listings from common ATS platforms
- Integrate with existing job storage mechanisms

### Improve Company Research Quality

- Add targeted research for specific data points like funding information
- Implement sentiment analysis on employee reviews
- Create better validation of AI-extracted structured data
- Build confidence scoring for information extracted from different sources

### Scale and Performance Optimization

- Implement rate limiting for external API calls
- Create a job queue for processing companies in batches
- Add more detailed logging and metrics tracking
- Optimize search and extraction processes for better time/cost efficiency

## References and Resources

### Internal References:

- CompanyProfile model: fartemis.companies.models.CompanyProfile
- CompanyResearchReferences model: fartemis.companies.models.CompanyResearchReferences
- CompanyResearchLog model: fartemis.companies.models.CompanyResearchLog
- Company research controller: fartemis.companies.controllers.CompanyResearchController
- Funding status constants: fartemis.companies.constants.FundingStatus
- Company research command: fartemis.companies.management.commands.research_company
- Careers page command: fartemis.companies.management.commands.find_careers_pages

### External References:

- LangChain React Agent: https://python.langchain.com/docs/integrations/tools/react_agent/
- Tavily Search API: https://python.langchain.com/docs/integrations/tools/tavily_search
- Claude API: https://docs.anthropic.com/claude/reference/getting-started-with-the-api
- LangSmith Tracing: https://docs.smith.langchain.com/tracing
- Anthropic Claude Models: https://docs.anthropic.com/claude/docs/models-overview